{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# PyTorch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import ConcatDataset, DataLoader, random_split, Subset\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# sklearn modules\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Image processing modules\n",
        "from PIL import Image\n",
        "\n",
        "# File and directory handling modules\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Progress bar and timing modules\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Data compression and request modules\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Optuna\n",
        "!pip install optuna\n",
        "import optuna\n",
        "\n",
        "# Other modules\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "REvHedK0eH4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCkN6rJzd78p"
      },
      "outputs": [],
      "source": [
        "# Setup path to a data folder\n",
        "data_path = Path('data/')\n",
        "image_path = data_path / 'chest_xray'\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f'{image_path} directory already exists... skipping download')\n",
        "else: \n",
        "  print(f'{image_path} does not exist, creating one...')\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download x-ray scans data\n",
        "with open(data_path / 'chest_xray.zip', 'wb') as f:\n",
        "  request = requests.get('https://github.com/eliaszpiotr/PneumoniaDetection/raw/main/data/chest_xray.zip')\n",
        "  print('Downloading data...')\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile(data_path / 'chest_xray.zip', 'r') as zip_ref:\n",
        "  print('Unzipping data...')\n",
        "  zip_ref.extractall(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to dataset folders\n",
        "train_dir = image_path / 'chest_xray/train'\n",
        "test_dir = image_path / 'chest_xray/test'\n",
        "val_dir = image_path / 'chest_xray/val'\n",
        "\n",
        "train_dir, test_dir, val_dir"
      ],
      "metadata": {
        "id": "bBCojrjceHaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = {\n",
        "    'dataset1': transforms.Compose([\n",
        "        transforms.Resize(255),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(translate=(0.05, 0.05), degrees=0),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'dataset2': transforms.Compose([\n",
        "        transforms.Resize(255),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(translate=(0.1, 0.05), degrees=10),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'dataset3': transforms.Compose([\n",
        "        transforms.Resize(255),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(translate=(0.08, 0.1), degrees=15),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "hCOY_VuaeTT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transforms(input_dir, transform, postfix):\n",
        "    \"\"\"\n",
        "    Apply specified transformations to images in the input directory and save the transformed images to the same directory.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Path to the directory containing the input images.\n",
        "        transform (function): Transformation function to be applied to the images.\n",
        "        postfix (str): String to append to the filename to indicate the transformation applied.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Only consider original images, not those that have been previously transformed\n",
        "    img_list = [img for img in os.listdir(input_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')) and 'aug_' not in img]\n",
        "\n",
        "    for i, img_name in enumerate(img_list):\n",
        "        img_path = input_dir / img_name\n",
        "        img = Image.open(img_path)\n",
        "        transformed_img = transform(img)\n",
        "        # Add the 'aug_' prefix to the filename to distinguish the augmented images from the original ones\n",
        "        save_path = input_dir / f\"aug_{postfix}_{img_name}\"\n",
        "        save_image(transformed_img, save_path)\n",
        "\n",
        "    print(f\"Saved {len(img_list)} transformed images to {input_dir}\")"
      ],
      "metadata": {
        "id": "ixIQJ8oBeXL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_and_save(input_dir, output_dir, num_images):\n",
        "    \"\"\"\n",
        "    Randomly select the specified number of images from the given directory and save them to the output directory.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Path to the directory containing the input images.\n",
        "        output_dir (str): Path to the directory where the selected images will be saved.\n",
        "        num_images (int): Number of images to select.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    img_list = [img for img in os.listdir(input_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]\n",
        "    combined_list = [input_dir / img for img in img_list]\n",
        "\n",
        "    selected_images = random.sample(combined_list, num_images)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for img_path in selected_images:\n",
        "        shutil.copy(img_path, output_dir / img_path.name)\n",
        "\n",
        "    print(f\"Selected {len(os.listdir(output_dir))} images to {output_dir}\")"
      ],
      "metadata": {
        "id": "kmBbLi9oeZ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the desired number of images\n",
        "DESIRED_IMAGES = 500\n",
        "\n",
        "# Create new directories for the selected data\n",
        "selected_normal_dir = Path(\"selected_data/NORMAL\")\n",
        "selected_pneumonia_dir = Path(\"selected_data/PNEUMONIA\")\n",
        "\n",
        "# Apply transformations to all images and save them to the same directory\n",
        "for transformer_name in ['dataset1', 'dataset2', 'dataset3']:\n",
        "    apply_transforms(normal_train_dir, transformer[transformer_name], transformer_name)\n",
        "\n",
        "# Select images from the combined original and transformed images and save them to the new directory\n",
        "select_and_save(normal_train_dir, selected_normal_dir, DESIRED_IMAGES)\n",
        "\n",
        "# Repeat the process for the pneumonia images\n",
        "for transformer_name in ['dataset1', 'dataset2', 'dataset3']:\n",
        "    apply_transforms(pneumonia_train_dir, transformer[transformer_name], transformer_name)\n",
        "\n",
        "select_and_save(pneumonia_train_dir, selected_pneumonia_dir, DESIRED_IMAGES)"
      ],
      "metadata": {
        "id": "qfAYyNslec0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/selected_data'"
      ],
      "metadata": {
        "id": "LyGIAOmwelj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup datasets\n",
        "train_dataset = ImageFolder(data_dir,transform=transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "]))"
      ],
      "metadata": {
        "id": "j6iuPM8geoj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting data\n",
        "all_indices = list(range(len(train_dataset)))\n",
        "train_indices, temp_indices = train_test_split(\n",
        "    all_indices,\n",
        "    test_size=0.3, # 30% of the data will be used as temporary (validation + test)\n",
        "    random_state=42,  # Set a random state to make the split deterministic\n",
        "    stratify=train_dataset.targets\n",
        ")\n",
        "\n",
        "val_indices, test_indices = train_test_split(\n",
        "    temp_indices,\n",
        "    test_size=0.5,  # Half of the temporary data will be used as validation, and the other half as test\n",
        "    random_state=42,\n",
        "    stratify=[train_dataset.targets[i] for i in temp_indices]\n",
        ")\n",
        "\n",
        "full_dataset = ConcatDataset([train_dataset, Subset(train_dataset, temp_indices)])\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset = Subset(full_dataset, val_indices)\n",
        "test_dataset = Subset(full_dataset, test_indices)"
      ],
      "metadata": {
        "id": "rcMc7IcTe0nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set DataLoader parameters\n",
        "batch_size = 128\n",
        "num_workers = os.cpu_count()\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "g0qTAH9We4qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int, \n",
        "                 kernel_size: int, stride: int, padding: int, dropout_rate: float):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_shape, hidden_units, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units * 2, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units * 2, hidden_units * 2, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_units * 2 * 7 * 7, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_units, output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7kZzJTzvf3m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # Suggest values of the hyperparameters using a trial object.\n",
        "    hidden_units = trial.suggest_int(\"hidden_units\", 8, 32)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 0.01, log=True)\n",
        "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5])\n",
        "    stride = trial.suggest_categorical(\"stride\", [1, 2])\n",
        "    padding = trial.suggest_categorical(\"padding\", [0, 1])\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
        "\n",
        "    model = Classifier(3, hidden_units, 10, kernel_size, stride, padding, dropout_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):  # use train_loader here\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        trial.report(running_loss, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "    return running_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "Cgw4R8mpgdtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(\"  Value: \", best_trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "ILOn4lMahf79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}