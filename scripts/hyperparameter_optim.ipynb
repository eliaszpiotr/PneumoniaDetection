{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization using Optuna"
      ],
      "metadata": {
        "id": "Uzl6N5ISp1ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "dRykzBoqpx6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# sklearn modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# File and directory handling modules\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Other modules\n",
        "import random\n",
        "\n",
        "# Image processing modules\n",
        "from PIL import Image\n",
        "\n",
        "# Optuna\n",
        "!pip install optuna\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "REvHedK0eH4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e7f967-7c52-4de7-b404-c94387ef16a5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.10.4)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading data from GitHub"
      ],
      "metadata": {
        "id": "A8-3qnczqE6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wCkN6rJzd78p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2ffdc1-3df7-4543-a3fb-21d39fa99e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/chest_xray directory already exists... skipping download\n",
            "Downloading data...\n",
            "Unzipping data...\n"
          ]
        }
      ],
      "source": [
        "# Setup path to a data folder\n",
        "data_path = Path('data/')\n",
        "image_path = data_path / 'chest_xray'\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f'{image_path} directory already exists... skipping download')\n",
        "else: \n",
        "  print(f'{image_path} does not exist, creating one...')\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download x-ray scans data\n",
        "with open(data_path / 'chest_xray.zip', 'wb') as f:\n",
        "  request = requests.get('https://github.com/eliaszpiotr/PneumoniaDetection/raw/main/data/chest_xray.zip')\n",
        "  print('Downloading data...')\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile(data_path / 'chest_xray.zip', 'r') as zip_ref:\n",
        "  print('Unzipping data...')\n",
        "  zip_ref.extractall(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up paths"
      ],
      "metadata": {
        "id": "Xrl-4lZgqKpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to dataset folders\n",
        "train_dir = image_path / 'chest_xray/train'\n",
        "test_dir = image_path / 'chest_xray/test'\n",
        "val_dir = image_path / 'chest_xray/val'\n",
        "\n",
        "train_dir, test_dir, val_dir"
      ],
      "metadata": {
        "id": "bBCojrjceHaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5980d3fe-0509-4b8a-cc11-c53b9379d807"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/chest_xray/chest_xray/train'),\n",
              " PosixPath('data/chest_xray/chest_xray/test'),\n",
              " PosixPath('data/chest_xray/chest_xray/val'))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup transformers"
      ],
      "metadata": {
        "id": "2ytgzoTsu5ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = {\n",
        "    'dataset1': transforms.Compose([\n",
        "        transforms.Resize(255),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(translate=(0.05, 0.05), degrees=0),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'dataset2': transforms.Compose([\n",
        "        transforms.Resize(255),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(translate=(0.1, 0.05), degrees=10),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'dataset3': transforms.Compose([\n",
        "        transforms.Resize(255),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(translate=(0.08, 0.1), degrees=15),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "hCOY_VuaeTT9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup paths\n",
        "pneumonia_train_dir = train_dir / 'PNEUMONIA'\n",
        "normal_train_dir = train_dir/ 'NORMAL'\n",
        "pneumonia_test_dir = test_dir/ 'PNEUMONIA'\n",
        "normal_test_dir = test_dir/ 'NORMAL'\n",
        "pneumonia_val_dir = val_dir / 'PNEUMONIA'\n",
        "normal_val_dir = val_dir/ 'NORMAL'"
      ],
      "metadata": {
        "id": "FlE_hFpYuvGY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating helping functions"
      ],
      "metadata": {
        "id": "9MqFNlhWu_Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transforms(input_dir, transform, postfix):\n",
        "    \"\"\"\n",
        "    Apply specified transformations to images in the input directory and save the transformed images to the same directory.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Path to the directory containing the input images.\n",
        "        transform (function): Transformation function to be applied to the images.\n",
        "        postfix (str): String to append to the filename to indicate the transformation applied.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Only consider original images, not those that have been previously transformed\n",
        "    img_list = [img for img in os.listdir(input_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')) and 'aug_' not in img]\n",
        "\n",
        "    for i, img_name in enumerate(img_list):\n",
        "        img_path = input_dir / img_name\n",
        "        img = Image.open(img_path)\n",
        "        transformed_img = transform(img)\n",
        "        # Add the 'aug_' prefix to the filename to distinguish the augmented images from the original ones\n",
        "        save_path = input_dir / f\"aug_{postfix}_{img_name}\"\n",
        "        save_image(transformed_img, save_path)\n",
        "\n",
        "    print(f\"Saved {len(img_list)} transformed images to {input_dir}\")"
      ],
      "metadata": {
        "id": "ixIQJ8oBeXL5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_and_save(input_dir, output_dir, num_images):\n",
        "    \"\"\"\n",
        "    Randomly select the specified number of images from the given directory and save them to the output directory.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Path to the directory containing the input images.\n",
        "        output_dir (str): Path to the directory where the selected images will be saved.\n",
        "        num_images (int): Number of images to select.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    img_list = [img for img in os.listdir(input_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]\n",
        "    combined_list = [input_dir / img for img in img_list]\n",
        "\n",
        "    selected_images = random.sample(combined_list, num_images)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for img_path in selected_images:\n",
        "        shutil.copy(img_path, output_dir / img_path.name)\n",
        "\n",
        "    print(f\"Selected {len(os.listdir(output_dir))} images to {output_dir}\")"
      ],
      "metadata": {
        "id": "kmBbLi9oeZ0C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appling transformers and sampling images"
      ],
      "metadata": {
        "id": "IMpeu9WnvEjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the desired number of images\n",
        "DESIRED_IMAGES = 500\n",
        "\n",
        "# Create new directories for the selected data\n",
        "selected_normal_dir = Path(\"selected_data/NORMAL\")\n",
        "selected_pneumonia_dir = Path(\"selected_data/PNEUMONIA\")\n",
        "\n",
        "# Apply transformations to all images and save them to the same directory\n",
        "for transformer_name in ['dataset1', 'dataset2', 'dataset3']:\n",
        "    apply_transforms(normal_train_dir, transformer[transformer_name], transformer_name)\n",
        "\n",
        "# Select images from the combined original and transformed images and save them to the new directory\n",
        "select_and_save(normal_train_dir, selected_normal_dir, DESIRED_IMAGES)\n",
        "\n",
        "# Repeat the process for the pneumonia images\n",
        "for transformer_name in ['dataset1', 'dataset2', 'dataset3']:\n",
        "    apply_transforms(pneumonia_train_dir, transformer[transformer_name], transformer_name)\n",
        "\n",
        "select_and_save(pneumonia_train_dir, selected_pneumonia_dir, DESIRED_IMAGES)"
      ],
      "metadata": {
        "id": "qfAYyNslec0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e65e686-91dc-4a16-b958-c57a58b2aa40"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1341 transformed images to data/chest_xray/chest_xray/train/NORMAL\n",
            "Saved 1341 transformed images to data/chest_xray/chest_xray/train/NORMAL\n",
            "Saved 1341 transformed images to data/chest_xray/chest_xray/train/NORMAL\n",
            "Selected 500 images to selected_data/NORMAL\n",
            "Saved 3875 transformed images to data/chest_xray/chest_xray/train/PNEUMONIA\n",
            "Saved 3875 transformed images to data/chest_xray/chest_xray/train/PNEUMONIA\n",
            "Saved 3875 transformed images to data/chest_xray/chest_xray/train/PNEUMONIA\n",
            "Selected 500 images to selected_data/PNEUMONIA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loaders"
      ],
      "metadata": {
        "id": "OSPXk87IvKFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/selected_data'"
      ],
      "metadata": {
        "id": "LyGIAOmwelj5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup datasets\n",
        "train_dataset = ImageFolder(data_dir,transform=transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "]))"
      ],
      "metadata": {
        "id": "j6iuPM8geoj2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making splits"
      ],
      "metadata": {
        "id": "BWS1Ghs0vMMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting data\n",
        "all_indices = list(range(len(train_dataset)))\n",
        "train_indices, temp_indices = train_test_split(\n",
        "    all_indices,\n",
        "    test_size=0.3, # 30% of the data will be used as temporary (validation + test)\n",
        "    random_state=42,  # Set a random state to make the split deterministic\n",
        "    stratify=train_dataset.targets\n",
        ")\n",
        "\n",
        "val_indices, test_indices = train_test_split(\n",
        "    temp_indices,\n",
        "    test_size=0.5,  # Half of the temporary data will be used as validation, and the other half as test\n",
        "    random_state=42,\n",
        "    stratify=[train_dataset.targets[i] for i in temp_indices]\n",
        ")\n",
        "\n",
        "full_dataset = ConcatDataset([train_dataset, Subset(train_dataset, temp_indices)])\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset = Subset(full_dataset, val_indices)\n",
        "test_dataset = Subset(full_dataset, test_indices)"
      ],
      "metadata": {
        "id": "rcMc7IcTe0nZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set DataLoader parameters\n",
        "batch_size = 128\n",
        "num_workers = os.cpu_count()\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "g0qTAH9We4qL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "AbUd8S7n7PeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int, \n",
        "                 kernel_size: int, stride: int, padding: int, dropout_rate: float):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_shape, hidden_units, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units * 2, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units * 2, hidden_units * 2, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_units * 2 * 7 * 7, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_units, output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SqfM-MOS4Rxq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective function"
      ],
      "metadata": {
        "id": "9LlUU-i3vie8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # Suggest values of the hyperparameters using a trial object.\n",
        "    hidden_units = trial.suggest_int(\"hidden_units\", 8, 32)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 0.01, log=True)\n",
        "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5])\n",
        "    stride = trial.suggest_categorical(\"stride\", [1, 2])\n",
        "    padding = trial.suggest_categorical(\"padding\", [0, 1])\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])  # Add batch_size as a hyperparameter\n",
        "\n",
        "    model = Classifier(1, hidden_units, 2, kernel_size, stride, padding, dropout_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Create train_loader with the suggested batch_size\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):  # use train_loader here\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        trial.report(running_loss, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "    return running_loss\n"
      ],
      "metadata": {
        "id": "Cgw4R8mpgdtK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching for best parameters"
      ],
      "metadata": {
        "id": "QqDSNcG6voTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(\"  Value: \", best_trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "ILOn4lMahf79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a677641-94d6-46f7-a4ab-0bb0791a534a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-15 15:05:51,393]\u001b[0m A new study created in memory with name: no-name-54810a2a-2741-482f-aeb6-87b046abaec0\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}